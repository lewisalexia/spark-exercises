{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d60fbed",
   "metadata": {},
   "source": [
    "# Spark 101"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9aac72f",
   "metadata": {},
   "source": [
    "All the basics! \n",
    "\n",
    "Slides: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf60ed",
   "metadata": {},
   "source": [
    "## Create Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862d9d17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/06/30 10:39:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "23/06/30 10:39:29 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "#import spark for python! \n",
    "import pyspark\n",
    "#create the spark session\n",
    "spark = pyspark.sql.SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb87f0",
   "metadata": {},
   "source": [
    "## Dataframe Basics\n",
    " - create dataframe: `spark.createdataframe`\n",
    " - see the results: `.show`\n",
    " - look at your data: `.describe`, `.dtypes`, `printSchema`\n",
    " - select & create columns: `.select`, `col`, `expr`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47265e4",
   "metadata": {},
   "source": [
    "### create dataframe from a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2244a93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "44dd41fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   n group\n",
       "0  0     c\n",
       "1  1     a\n",
       "2  2     a\n",
       "3  3     a\n",
       "4  4     a"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(452)\n",
    "\n",
    "pandas_dataframe = pd.DataFrame(\n",
    "    dict(n=np.arange(20), group=np.random.choice(list(\"abc\"), 20))\n",
    ")\n",
    "\n",
    "pandas_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "240fbb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create spark dataframe \n",
    "df = spark.createDataFrame(pandas_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443911b5",
   "metadata": {},
   "source": [
    "### See the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5716241",
   "metadata": {},
   "source": [
    "<div class='alert alert-box alert-info'>\n",
    "<b>Note:</b> Spark is lazy and won't display info unless you tell it to\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d3c656d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "|  n|group|\n",
      "+---+-----+\n",
      "|  0|    c|\n",
      "|  1|    a|\n",
      "|  2|    a|\n",
      "|  3|    a|\n",
      "|  4|    a|\n",
      "|  5|    a|\n",
      "|  6|    c|\n",
      "|  7|    c|\n",
      "|  8|    a|\n",
      "|  9|    a|\n",
      "| 10|    b|\n",
      "| 11|    a|\n",
      "| 12|    a|\n",
      "| 13|    a|\n",
      "| 14|    a|\n",
      "| 15|    a|\n",
      "| 16|    b|\n",
      "| 17|    a|\n",
      "| 18|    c|\n",
      "| 19|    a|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#one way to pull in a spark df\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb01a78",
   "metadata": {},
   "source": [
    "### create dataframe from the pydataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "967f1774",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydataset import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4eb319e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[manufacturer: string, model: string, displ: double, year: bigint, cyl: bigint, trans: string, drv: string, cty: bigint, hwy: bigint, fl: string, class: string]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame(data('mpg'))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f61b764",
   "metadata": {},
   "source": [
    "### look at your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e613003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|             model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|                a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|                a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|                a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|                a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|                a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "|        audi|                a4|  2.8|1999|  6|manual(m5)|  f| 18| 26|  p|compact|\n",
      "|        audi|                a4|  3.1|2008|  6|  auto(av)|  f| 18| 27|  p|compact|\n",
      "|        audi|        a4 quattro|  1.8|1999|  4|manual(m5)|  4| 18| 26|  p|compact|\n",
      "|        audi|        a4 quattro|  1.8|1999|  4|  auto(l5)|  4| 16| 25|  p|compact|\n",
      "|        audi|        a4 quattro|  2.0|2008|  4|manual(m6)|  4| 20| 28|  p|compact|\n",
      "|        audi|        a4 quattro|  2.0|2008|  4|  auto(s6)|  4| 19| 27|  p|compact|\n",
      "|        audi|        a4 quattro|  2.8|1999|  6|  auto(l5)|  4| 15| 25|  p|compact|\n",
      "|        audi|        a4 quattro|  2.8|1999|  6|manual(m5)|  4| 17| 25|  p|compact|\n",
      "|        audi|        a4 quattro|  3.1|2008|  6|  auto(s6)|  4| 17| 25|  p|compact|\n",
      "|        audi|        a4 quattro|  3.1|2008|  6|manual(m6)|  4| 15| 25|  p|compact|\n",
      "|        audi|        a6 quattro|  2.8|1999|  6|  auto(l5)|  4| 15| 24|  p|midsize|\n",
      "|        audi|        a6 quattro|  3.1|2008|  6|  auto(s6)|  4| 17| 25|  p|midsize|\n",
      "|        audi|        a6 quattro|  4.2|2008|  8|  auto(s6)|  4| 16| 23|  p|midsize|\n",
      "|   chevrolet|c1500 suburban 2wd|  5.3|2008|  8|  auto(l4)|  r| 14| 20|  r|    suv|\n",
      "|   chevrolet|c1500 suburban 2wd|  5.3|2008|  8|  auto(l4)|  r| 11| 15|  e|    suv|\n",
      "+------------+------------------+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to see the entire df\n",
    "df.show() #default is 20, can change by inputting an integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c935cce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/06/30 10:42:30 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 14:>                                                       (0 + 16) / 16]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-----------------+------------------+-----------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "|summary|manufacturer|            model|             displ|             year|              cyl|     trans|drv|               cty|              hwy|  fl|  class|\n",
      "+-------+------------+-----------------+------------------+-----------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "|  count|         234|              234|               234|              234|              234|       234|234|               234|              234| 234|    234|\n",
      "|   mean|        null|             null| 3.471794871794872|           2003.5|5.888888888888889|      null|4.0|16.858974358974358|23.44017094017094|null|   null|\n",
      "| stddev|        null|             null|1.2919590310839348|4.509646313320452|1.611534484684289|      null|0.0| 4.255945678889394|5.954643441166446|null|   null|\n",
      "|    min|        audi|      4runner 4wd|               1.6|             1999|                4|  auto(av)|  4|                 9|               12|   c|2seater|\n",
      "|    max|  volkswagen|toyota tacoma 4wd|               7.0|             2008|                8|manual(m6)|  r|                35|               44|   r|    suv|\n",
      "+-------+------------+-----------------+------------------+-----------------+-----------------+----------+---+------------------+-----------------+----+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#describe\n",
    "df.describe().show() #too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bed94c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0--------------------------\n",
      " summary      | count              \n",
      " manufacturer | 234                \n",
      " model        | 234                \n",
      " displ        | 234                \n",
      " year         | 234                \n",
      " cyl          | 234                \n",
      " trans        | 234                \n",
      " drv          | 234                \n",
      " cty          | 234                \n",
      " hwy          | 234                \n",
      " fl           | 234                \n",
      " class        | 234                \n",
      "-RECORD 1--------------------------\n",
      " summary      | mean               \n",
      " manufacturer | null               \n",
      " model        | null               \n",
      " displ        | 3.471794871794872  \n",
      " year         | 2003.5             \n",
      " cyl          | 5.888888888888889  \n",
      " trans        | null               \n",
      " drv          | 4.0                \n",
      " cty          | 16.858974358974358 \n",
      " hwy          | 23.44017094017094  \n",
      " fl           | null               \n",
      " class        | null               \n",
      "-RECORD 2--------------------------\n",
      " summary      | stddev             \n",
      " manufacturer | null               \n",
      " model        | null               \n",
      " displ        | 1.2919590310839348 \n",
      " year         | 4.509646313320452  \n",
      " cyl          | 1.611534484684289  \n",
      " trans        | null               \n",
      " drv          | 0.0                \n",
      " cty          | 4.255945678889394  \n",
      " hwy          | 5.954643441166446  \n",
      " fl           | null               \n",
      " class        | null               \n",
      "-RECORD 3--------------------------\n",
      " summary      | min                \n",
      " manufacturer | audi               \n",
      " model        | 4runner 4wd        \n",
      " displ        | 1.6                \n",
      " year         | 1999               \n",
      " cyl          | 4                  \n",
      " trans        | auto(av)           \n",
      " drv          | 4                  \n",
      " cty          | 9                  \n",
      " hwy          | 12                 \n",
      " fl           | c                  \n",
      " class        | 2seater            \n",
      "-RECORD 4--------------------------\n",
      " summary      | max                \n",
      " manufacturer | volkswagen         \n",
      " model        | toyota tacoma 4wd  \n",
      " displ        | 7.0                \n",
      " year         | 2008               \n",
      " cyl          | 8                  \n",
      " trans        | manual(m6)         \n",
      " drv          | r                  \n",
      " cty          | 35                 \n",
      " hwy          | 44                 \n",
      " fl           | r                  \n",
      " class        | suv                \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#describe vertical\n",
    "df.describe().show(vertical=True) #works like transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "76d8c475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- displ: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- cyl: long (nullable = true)\n",
      " |-- trans: string (nullable = true)\n",
      " |-- drv: string (nullable = true)\n",
      " |-- cty: long (nullable = true)\n",
      " |-- hwy: long (nullable = true)\n",
      " |-- fl: string (nullable = true)\n",
      " |-- class: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/26/y7jmy96d6tx38pgw7pxw_dpm0000gn/T/ipykernel_10824/2355315920.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#printSchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprintSchema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#gives column names and what datatype it is and if it can have null values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "#printSchema\n",
    "df.printSchema().show() #gives column names and what datatype it is and if it can have null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d30ce2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manufacturer', 'string'),\n",
       " ('model', 'string'),\n",
       " ('displ', 'double'),\n",
       " ('year', 'bigint'),\n",
       " ('cyl', 'bigint'),\n",
       " ('trans', 'string'),\n",
       " ('drv', 'string'),\n",
       " ('cty', 'bigint'),\n",
       " ('hwy', 'bigint'),\n",
       " ('fl', 'string'),\n",
       " ('class', 'string')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dtypes\n",
    "df.dtypes #gives dtype on its own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "deb81bac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "234"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of rows\n",
    "df.count() #how to get shape in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d9a12530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#number of columns\n",
    "len(df.columns) #how to get shape in spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9711fe08",
   "metadata": {},
   "source": [
    "### select columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb1fc314",
   "metadata": {},
   "source": [
    "<div class='alert alert-box alert-info'>\n",
    "<b>Reminder:</b> Spark is still lazy\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a7251da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     trans|\n",
      "+----------+\n",
      "|  auto(l5)|\n",
      "|manual(m5)|\n",
      "|manual(m6)|\n",
      "|  auto(av)|\n",
      "|  auto(l5)|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#looking at a column\n",
    "df.select('trans').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "67dd1e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|     trans|\n",
      "+----------+\n",
      "|  auto(l5)|\n",
      "|manual(m5)|\n",
      "|manual(m6)|\n",
      "|  auto(av)|\n",
      "|  auto(l5)|\n",
      "+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#another way\n",
    "df.select(df.trans).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "00d9d954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|     trans|hwy|\n",
      "+----------+---+\n",
      "|  auto(l5)| 29|\n",
      "|manual(m5)| 29|\n",
      "|manual(m6)| 31|\n",
      "|  auto(av)| 30|\n",
      "|  auto(l5)| 26|\n",
      "+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select two columns\n",
    "df.select('trans', 'hwy').show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b157c06e",
   "metadata": {},
   "source": [
    "#### save our selected columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f30cf",
   "metadata": {},
   "source": [
    "<div class='alert alert-box alert-info'>\n",
    "<b>Note:</b> You can not save a display output\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "84e02b42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+\n",
      "|             model|year|\n",
      "+------------------+----+\n",
      "|                a4|1999|\n",
      "|                a4|1999|\n",
      "|                a4|2008|\n",
      "|                a4|2008|\n",
      "|                a4|1999|\n",
      "|                a4|1999|\n",
      "|                a4|2008|\n",
      "|        a4 quattro|1999|\n",
      "|        a4 quattro|1999|\n",
      "|        a4 quattro|2008|\n",
      "|        a4 quattro|2008|\n",
      "|        a4 quattro|1999|\n",
      "|        a4 quattro|1999|\n",
      "|        a4 quattro|2008|\n",
      "|        a4 quattro|2008|\n",
      "|        a6 quattro|1999|\n",
      "|        a6 quattro|2008|\n",
      "|        a6 quattro|2008|\n",
      "|c1500 suburban 2wd|2008|\n",
      "|c1500 suburban 2wd|2008|\n",
      "+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#save this as a variable and do something with it\n",
    "df_cols = df.select('model','year').show() #this df did not save the DF inside the VARIABLE because of the .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e92ff76a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'show'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/26/y7jmy96d6tx38pgw7pxw_dpm0000gn/T/ipykernel_10824/3949250332.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#trying to look at the variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_cols\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'show'"
     ]
    }
   ],
   "source": [
    "#trying to look at the variable\n",
    "df_cols.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ba1d3670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NoneType"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#type\n",
    "type(df_cols) #because of that .show() -- spark only visualizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bbf4f5ea-43d4-4e5a-859d-67c6d5b8bdfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----+\n",
      "|             model|year|\n",
      "+------------------+----+\n",
      "|                a4|1999|\n",
      "|                a4|1999|\n",
      "|                a4|2008|\n",
      "|                a4|2008|\n",
      "|                a4|1999|\n",
      "|                a4|1999|\n",
      "|                a4|2008|\n",
      "|        a4 quattro|1999|\n",
      "|        a4 quattro|1999|\n",
      "|        a4 quattro|2008|\n",
      "|        a4 quattro|2008|\n",
      "|        a4 quattro|1999|\n",
      "|        a4 quattro|1999|\n",
      "|        a4 quattro|2008|\n",
      "|        a4 quattro|2008|\n",
      "|        a6 quattro|1999|\n",
      "|        a6 quattro|2008|\n",
      "|        a6 quattro|2008|\n",
      "|c1500 suburban 2wd|2008|\n",
      "|c1500 suburban 2wd|2008|\n",
      "+------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#remove the .show()\n",
    "df_cols = df.select('model','year')\n",
    "df_cols.show() #now it is saved into a variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e399b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### create columns\n",
    " - use basic math operators\n",
    " - change column names: `alias`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "35911678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0567d4b0",
   "metadata": {},
   "source": [
    "#### half the highway mileage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ddf22e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'(hwy / 2)'>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create transformation\n",
    "df.hwy / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "f7dcbc7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|(hwy / 2)|\n",
      "+---------+\n",
      "|     14.5|\n",
      "|     14.5|\n",
      "|     15.5|\n",
      "|     15.0|\n",
      "|     13.0|\n",
      "+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create action\n",
    "df.select(df.hwy / 2).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4fc66792-8024-4675-9f61-fb50e10efc77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------+\n",
      "|hwy|half_hwy|\n",
      "+---+--------+\n",
      "| 29|    14.5|\n",
      "| 29|    14.5|\n",
      "| 31|    15.5|\n",
      "| 30|    15.0|\n",
      "| 26|    13.0|\n",
      "+---+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#create alias for column name\n",
    "df.select(df.hwy, (df.hwy/2).alias('half_hwy')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6538e949",
   "metadata": {},
   "source": [
    "### select & create columns: `col`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9017e346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "41b65add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+---------+------------+\n",
      "|hwy|hwy|(hwy + 1)|hwy_and_city|\n",
      "+---+---+---------+------------+\n",
      "| 29| 29|       30|          47|\n",
      "| 29| 29|       30|          50|\n",
      "| 31| 31|       32|          51|\n",
      "| 30| 30|       31|          51|\n",
      "| 26| 26|       27|          42|\n",
      "+---+---+---------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.hwy,\n",
    "          col('hwy'),\n",
    "          col('hwy') + 1,\n",
    "          (col('hwy') + col('cty')).alias('hwy_and_city')).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692787c6",
   "metadata": {},
   "source": [
    "### select & create columns: `expr`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "eab57536",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "012ee395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+\n",
      "|hwy|(hwy + 1)|\n",
      "+---+---------+\n",
      "| 29|       30|\n",
      "| 29|       30|\n",
      "| 31|       32|\n",
      "| 30|       31|\n",
      "| 26|       27|\n",
      "+---+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\n",
    "    expr('hwy'),\n",
    "    expr('hwy + 1')\n",
    "         \n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad326894",
   "metadata": {},
   "source": [
    "### create column: `withColumn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "cbea5bc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+------------+\n",
      "|manufacturer|model|displ|year|cyl|     trans|drv|cty|hwy| fl|  class|hwy_plus_one|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+------------+\n",
      "|        audi|   a4|  1.8|1999|  4|  auto(l5)|  f| 18| 29|  p|compact|          30|\n",
      "|        audi|   a4|  1.8|1999|  4|manual(m5)|  f| 21| 29|  p|compact|          30|\n",
      "|        audi|   a4|  2.0|2008|  4|manual(m6)|  f| 20| 31|  p|compact|          32|\n",
      "|        audi|   a4|  2.0|2008|  4|  auto(av)|  f| 21| 30|  p|compact|          31|\n",
      "|        audi|   a4|  2.8|1999|  6|  auto(l5)|  f| 16| 26|  p|compact|          27|\n",
      "+------------+-----+-----+----+---+----------+---+---+---+---+-------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#keeps the original\n",
    "df.withColumn(\n",
    "    'hwy_plus_one', #new column name\n",
    "              col('hwy') + 1).show(5) #how the new column is made"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcaf2486",
   "metadata": {},
   "source": [
    "## Transforming columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5c1ed",
   "metadata": {},
   "source": [
    "### built-in functions - math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "de955cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import min, max, sum, count, mean, avg\n",
    "#import pyspark.sql.functions as F #also common import! call as F.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "8846ec82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-----------------+-----------------+----------+\n",
      "|min(hwy)|max(hwy)|         avg(hwy)|         avg(hwy)|count(hwy)|\n",
      "+--------+--------+-----------------+-----------------+----------+\n",
      "|      12|      44|23.44017094017094|23.44017094017094|       234|\n",
      "+--------+--------+-----------------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#use min, max, and calculate average highway mileage\n",
    "df.select(\n",
    "    min(df.hwy), max(df.hwy), avg(df.hwy), mean(df.hwy), count(col('hwy')) #these are all only one value\n",
    "                                    #these operate the exact same way\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d181f0",
   "metadata": {},
   "source": [
    "### built-in functions - strings\n",
    "- `concat`: to concatenate strings\n",
    "- `lit`: creates literal value of character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8af32aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import concat, lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ee0618af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|concat(manufacturer, model)|\n",
      "+---------------------------+\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "|                     audia4|\n",
      "+---------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#combine manufacturer and model together\n",
    "df.select(\n",
    "    concat(df.manufacturer, df.model)\n",
    "\n",
    ").show(5) #smushed together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "71e7f031",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+\n",
      "|concat(manufacturer,  , model)|\n",
      "+------------------------------+\n",
      "|                       audi a4|\n",
      "|                       audi a4|\n",
      "|                       audi a4|\n",
      "|                       audi a4|\n",
      "|                       audi a4|\n",
      "+------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#get a space\n",
    "df.select(\n",
    "    concat(df.manufacturer, lit(' '), df.model) #using lit to get a LITERAL space\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d7f32a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------------------+\n",
      "|cty|hwy|concat(cty,  , hwy)|\n",
      "+---+---+-------------------+\n",
      "| 18| 29|              18 29|\n",
      "| 21| 29|              21 29|\n",
      "| 20| 31|              20 31|\n",
      "| 21| 30|              21 30|\n",
      "| 16| 26|              16 26|\n",
      "+---+---+-------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#combine city and highway together\n",
    "df.select(\n",
    "    df.cty, \n",
    "    df.hwy,\n",
    "    concat(df.cty, lit(' '), df.hwy) #concat treats things as strings   #if you want to add use the operator\n",
    "\n",
    ").show(5) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f63b83",
   "metadata": {},
   "source": [
    "### Regex! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187bd9ad",
   "metadata": {},
   "source": [
    "- `regexp_extract`: use regex to extract data\n",
    "- `regexp_replace`: use regex to replace data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "ca9d0fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5b6a28ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+\n",
      "|address                                        |\n",
      "+-----------------------------------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205  |\n",
      "|3130 Broadway St, San Antonio, TX 78209        |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215          |\n",
      "|1255 SW Loop 410!!!!, San - Antonio, TX @78227@|\n",
      "+-----------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "textdf = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"address\": [\n",
    "                \"600 Navarro St ste 600, San Antonio, TX 78205\",\n",
    "                \"3130 Broadway St, San Antonio, TX 78209\",\n",
    "                \"303 Pearl Pkwy, San Antonio, TX 78215\",\n",
    "                \"1255 SW Loop 410!!!!, San - Antonio, TX @78227@\",\n",
    "            ]\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "textdf.show(truncate=False) #allows us to see the full cell!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "b3063034",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------+----------+-----------+-------------------------------------------+\n",
      "|address                                        |street_num|street_name|clean_address                              |\n",
      "+-----------------------------------------------+----------+-----------+-------------------------------------------+\n",
      "|600 Navarro St ste 600, San Antonio, TX 78205  |600       |600        |600 Navarro St ste 600 San Antonio TX 78205|\n",
      "|3130 Broadway St, San Antonio, TX 78209        |3130      |3130       |3130 Broadway St San Antonio TX 78209      |\n",
      "|303 Pearl Pkwy, San Antonio, TX 78215          |303       |303        |303 Pearl Pkwy San Antonio TX 78215        |\n",
      "|1255 SW Loop 410!!!!, San - Antonio, TX @78227@|1255      |1255       |1255 SW Loop 410 San  Antonio TX 78227     |\n",
      "+-----------------------------------------------+----------+-----------+-------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#cleaning the strings using regexp\n",
    "textdf.select(\n",
    "    'address',\n",
    "    regexp_extract('address', r'\\d+', 0).alias('street_num'),\n",
    "    regexp_extract('address', r'(\\d+)\\s(\\w+)', 1).alias('street_name'),\n",
    "    #replace exclamation points\n",
    "    regexp_replace('address', r'[^\\w\\s]', '').alias('clean_address')\n",
    "\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3d7207",
   "metadata": {},
   "source": [
    "### Filter and Where"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b67619",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(data('mpg'))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7387beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86b2092c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b5e8f8d",
   "metadata": {},
   "source": [
    "### When and Otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d59a9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc4856a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.select(\n",
    "\n",
    ").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf6d2b",
   "metadata": {},
   "source": [
    "### sorting and ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab4c048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc89599f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff4b0d1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d13d22a8",
   "metadata": {},
   "source": [
    "### Grouping and Aggregating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14236323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby/groupBy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4947e026",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rollup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db2320c",
   "metadata": {},
   "source": [
    "### Crosstabs and Pivot Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35825907",
   "metadata": {},
   "outputs": [],
   "source": [
    "#crosstab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ad0409",
   "metadata": {},
   "outputs": [],
   "source": [
    "#groupby and pivot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a378581e",
   "metadata": {},
   "source": [
    "### Handling Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e3a44",
   "metadata": {},
   "source": [
    "- `.na.fill`: to replace missing values with a specified value\n",
    "- `.na.drop`: to drop rows containing missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce7829",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\"x\": [1, 2, np.nan, 4, 5, np.nan], \"y\": [np.nan, 0, 0, 3, 1, np.nan]}\n",
    "    )\n",
    ")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3855f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9618d172",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2ecd9a30",
   "metadata": {},
   "source": [
    "### More Dataframe Manipulation Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d050437",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vega_datasets import data\n",
    "\n",
    "weather = data.seattle_weather().assign(date=lambda df: df.date.astype(str))\n",
    "df = spark.createDataFrame(weather)\n",
    "df.show(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8f3b1",
   "metadata": {},
   "source": [
    "#### shape of df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73927d93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4c20802",
   "metadata": {},
   "source": [
    "#### start and end date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd58bc78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad8f50a5",
   "metadata": {},
   "source": [
    "#### Find the total rainfall per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e36ed35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import month, year, quarter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c87491",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c05c52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "659580d8",
   "metadata": {},
   "source": [
    "### Joins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b59219",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    df.withColumn(\"month\", month(\"date\"))\n",
    "    .groupBy(\"month\")\n",
    "    .agg(sum(\"precipitation\").alias(\"total_rainfall\"))\n",
    "    .sort(\"month\")\n",
    "    .show()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2613300e",
   "metadata": {},
   "outputs": [],
   "source": [
    "users = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4, 5, 6],\n",
    "            \"name\": [\"bob\", \"joe\", \"sally\", \"adam\", \"jane\", \"mike\"],\n",
    "            \"role_id\": [1, 2, 3, 3, np.nan, np.nan],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "roles = spark.createDataFrame(\n",
    "    pd.DataFrame(\n",
    "        {\n",
    "            \"id\": [1, 2, 3, 4],\n",
    "            \"name\": [\"admin\", \"author\", \"reviewer\", \"commenter\"],\n",
    "        }\n",
    "    )\n",
    ")\n",
    "print(\"--- users ---\")\n",
    "users.show()\n",
    "print(\"--- roles ---\")\n",
    "roles.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e106c88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd81177",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2887a1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e348be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "users.join(roles, on=users.role_id == roles.id).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef9d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "users.join(roles, on=users.role_id == roles.id, how=\"left\").show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
